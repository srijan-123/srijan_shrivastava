# -*- coding: utf-8 -*-
"""Text_Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13xnLnKEdaGxcy0egknjCT7hZtfZl9xrh
"""

import os
import pandas as pd
import re
import time
import glob
import requests
import concurrent.futures
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from bs4 import BeautifulSoup as bs

CURRENT_WORKING_DIRECTORY = '/kaggle/input/'
OUTPUT_DIRECTORY = '/kaggle/working/'

"""# ***Extracting input-data in to pandas data frame:***"""

#Reading input file:
data = pd.read_excel(CURRENT_WORKING_DIRECTORY + 'input-data/Input_data.xlsx')
data.head(10)

"""# ***Data Extraction:***

# ***Using BeautifulSoup to extract data from the urls:***

**We are using multithreading here in order to speed up the process of scrapping data from all the urls as the time taken to complete this step while calling one url at a time was approximately around 50.23 seconds while using multithreading the time taken for the whole data scrapping process to complete was approximately around 12.3 seconds thus reducing the time taken significantly.**
"""

st = time.time()

def scraped_content(url):
    #print(url)
    response = requests.get(url)
    soup = bs(response.content, 'html.parser')
    text_div = soup.select("div.td-post-content > p")
    #print(text_div)
    text_lst = []
    for j in text_div:
        text_lst.append(j.text)
    #print(text_lst)
    words = " ".join(text_lst).lower()    #words contain complete single article in string data-type
    #print(words)
    return words

url_lst = data['URL'].values              #Providing the required URLs for extraction
    
with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
    results = executor.map(scraped_content, url_lst)
#print(results)
print(time.time() - st)

allarticles_lst = list(results)   #Saving all the text files from different articles

#print(allarticles_lst[0:2])

"""# ***Saving extracted articles in a text file with URL_ID as its name:***"""

for i in range(len(data['URL_ID'])):
    filename = str(data['URL_ID'][i])
    f = open(filename + '.txt',"w+")
    text = str(allarticles_lst[i])
    f.write(text)
    f.close()

"""# ***Extracting all stopwords from different stopword files provided in Stopwords folder:***

**The Stop Words Lists are used to clean the text so that Sentiment Analysis can be performed by excluding the words found in Stop Words List.**
"""

stopwords_lst = []
lst1 = []
for name in glob.glob(CURRENT_WORKING_DIRECTORY + 'stopwords/*'):
    #print(name)
    f = open(name, "r", errors='ignore')
    #print(f)
    stopwords_lst.append(f.read().lower())

stopwords_lst_final = []
for i in range(len(stopwords_lst)):
    x = stopwords_lst[i].split("\n")  #removing the '\n' character from all the stopwords and splitting all the words from that character
    x = [i.rstrip() for i in x]       #removing whitespace after each stopwords in the list
    stopwords_lst_final.extend(x)
stopwords_lst_final = set(stopwords_lst_final)

"""# ***Function for Normalizing all the text files before calculating the output variables:***"""

contractions_dict = { "ain't": "are not", "'s":"is", "aren't": "are not", "can't": "cannot",
                     "can't've": "cannot have", "‘cause": "because", "could've": "could have",
                     "couldn't": "could not", "couldn't've": "could not have", "didn't": "did not",
                     "doesn't": "does not", "don't": "do not", "hadn't": "had not", "hadn't've": "had not have",
                     "hasn't": "has not", "haven't": "have not", "he'd": "he would", "he'd've": "he would have",
                     "he'll": "he will", "he'll've": "he will have", "how'd": "how did", "how'd'y": "how do you",
                     "how'll": "how will", "i'd": "i would", "i'd've": "i would have", "i'll": "i will",
                     "i'll've": "i will have", "i'm": "i am", "i've": "i have", "isn't": "is not",
                     "it'd": "it would", "it'd've": "it would have", "it'll": "it will", "it'll've": "it will have",
                     "let's": "let us", "ma'am": "madam", "mayn't": "may not", "might've": "might have",
                     "mightn't": "might not", "mightn't've": "might not have", "must've": "must have", 
                     "mustn't": "must not", "mustn't've": "must not have", "needn't": "need not", 
                     "needn't've": "need not have", "o'clock": "of the clock", "oughtn't": "ought not",
                     "oughtn't've": "ought not have", "shan't": "shall not", "sha'n't": "shall not",
                     "shan't've": "shall not have", "she'd": "she would", "she'd've": "she would have", 
                     "she'll": "she will", "she'll've": "she will have", "should've": "should have",
                     "shouldn't": "should not", "shouldn't've": "should not have", "so've": "so have",
                     "that'd": "that would", "that'd've": "that would have", "there'd": "there would", 
                     "there'd've": "there would have", "they'd": "they would", "they'd've": "they would have",
                     "they'll": "they will", "they'll've": "they will have", "they're": "they are",
                     "they've": "they have", "to've": "to have", "wasn't": "was not", "we'd": "we would",
                     "we'd've": "we would have", "we'll": "we will", "we'll've": "we will have", "we're": "we are",
                     "we've": "we have", "weren't": "were not","what'll": "what will", 
                     "what'll've": "what will have", "what're": "what are", "what've": "what have",
                     "when've": "when have", "where'd": "where did", "where've": "where have", "who'll": "who will",
                     "who'll've": "who will have", "who've": "who have", "why've": "why have",
                     "will've": "will have", "won't": "will not", "won't've": "will not have",
                     "would've": "would have", "wouldn't": "would not", "wouldn't've": "would not have",
                     "y'all": "you all", "y'all'd": "you all would", "y'all'd've": "you all would have",
                     "y'all're": "you all are", "y'all've": "you all have", "you'd": "you would",
                     "you'd've": "you would have", "you'll": "you will", "you'll've": "you will have", 
                     "you're": "you are", "you've": "you have", "what's": "what is"}

def text_norm(documents):
    return_documents=[]
    def expand_contractions(s, contractions_dict=contractions_dict):
        def replace(match):
            return contractions_dict[match.group(0)]
        return contractions_re.sub(replace, s)
    contractions_re = re.compile('(%s)'%'|'.join(contractions_dict.keys()))
    for line in documents:
        line_return = line.lower()
        line_return = expand_contractions(line_return)
        line_return = re.sub(r'[^\w\s]', '', line_return)
        return_documents.append(line_return)
    return ''.join(return_documents)

"""# ***Creating Article List:***

**Creating a list of articles from all the saved text files:**
"""

article_lst = []
for url_id in data['URL_ID']:
    #print(url_id)
    name = OUTPUT_DIRECTORY + str(url_id) + '.txt'
    f = open(name, 'r')
    documents = f.read()
    article_lst.append(documents)

#print(article_lst[0])

"""# ***1  Sentimental Analysis:***

# ***1.1  Removing Stopwords from articles using Stopwords Lists:***
"""

def stopwords_lists(stopwords_lst_final,article_lst):
    article_wo_stopwords = []
    for article in article_lst:
        article_norm_txt = text_norm(article)
        article_lst_norm = article_norm_txt.split(' ')
        wo_stopwords = []
        for word in article_lst_norm:
            if word not in stopwords_lst_final:
                wo_stopwords.append(word)
        article_wo_stopwords.append(wo_stopwords)
    return article_wo_stopwords

articles_wo_stopwords_lst = stopwords_lists(stopwords_lst_final, article_lst)

#print(articles_wo_stopwords_lst)

"""# ***1.2  Creating the master dictionary for postive and negative words:***

**The Master Dictionary is used for creating a dictionary of Positive and Negative words. We add only those words in the dictionary if they are not found in the Stop Words Lists.**
"""

negative = open(CURRENT_WORKING_DIRECTORY + 'master-dictionary/negative-words.txt', 'r', errors = 'ignore')
negative_words_lst = negative.read().split("\n")
    
positive = open(CURRENT_WORKING_DIRECTORY + 'master-dictionary/positive-words.txt', 'r', errors = 'ignore')
positive_words_lst = positive.read().split("\n")

def master_dictionary(positive_words_lst, negative_words_lst, articles_wo_stopwords_lst):
    master_dict = {}
    positive_words_lst_article_wise = []
    negative_words_lst_article_wise = []
    for article in articles_wo_stopwords_lst:
        positive_words = []
        negative_words = []        
        for words in article:
            if words in positive_words_lst:
                positive_words.append(words)
            elif words in negative_words_lst:
                negative_words.append(words)
        positive_words_lst_article_wise.append(positive_words)
        negative_words_lst_article_wise.append(negative_words)
        master_dict['positive_words_articlewise'] = [words_articlewise for words_articlewise in positive_words_lst_article_wise]
        master_dict['negative_words_articlewise'] = [words_articlewise for words_articlewise in negative_words_lst_article_wise]
    return master_dict

master_dict = master_dictionary(positive_words_lst, negative_words_lst, articles_wo_stopwords_lst)

#print(master_dict)

"""# ***1.3  Extracting Derived variables:***

# ***Positive Score:***

**This score is calculated by assigning the value of +1 for each word if found in the Positive Dictionary and then adding up all the values.**
"""

def positive_score(master_dict):
    positive_score_lst_articlewise = []
    for positive_words_articlewise in master_dict['positive_words_articlewise']:
        count = len(positive_words_articlewise)
        positive_score_lst_articlewise.append(count)
    return positive_score_lst_articlewise

positive_score_lst_articlewise = positive_score(master_dict)

#print(positive_score_lst_articlewise)

"""# ***Negative Score:***

**This score is calculated by assigning the value of -1 for each word if found in the Negative Dictionary and then adding up all the values. We multiply the score with -1 so that the score is a positive number.**

'negative_words_articlewise' contains number of lists where each list belong to a single article containing all the negative words present in that particular article, thus for calculating the number of negative words present in that article we are taking the length of that particular list(negative_words_articlewise).
"""

def negative_score(master_dict):
    negative_score_lst_articlewise = []
    for negative_words_articlewise in master_dict['negative_words_articlewise']:
        count = len(negative_words_articlewise)
        negative_score_lst_articlewise.append(count)
    return negative_score_lst_articlewise

negative_score_lst_articlewise = negative_score(master_dict)

#print(negative_score_lst_articlewise)

"""# ***Polarity Score(Range is from -1 to +1):***

**This is the score that determines if a given text is positive or negative in nature. It is calculated by using the formula:**

**Polarity Score = (Positive Score – Negative Score)/ ((Positive Score + Negative Score) + 0.000001)**
"""

def polarity_score(positive_score_lst_articlewise, negative_score_lst_articlewise):
    polarity_score_lst_articlewise = []
    for score in range(len(positive_score_lst_articlewise)):
        polarity_score = (positive_score_lst_articlewise[score] - negative_score_lst_articlewise[score]) / ((positive_score_lst_articlewise[score] + negative_score_lst_articlewise[score]) + 0.000001)
        #print(polarity_score)
        polarity_score_lst_articlewise.append(polarity_score)
    return polarity_score_lst_articlewise

polarity_score_lst_articlewise = polarity_score(positive_score_lst_articlewise, negative_score_lst_articlewise)

#print(polarity_score_lst_articlewise)

"""# ***Subjectivity Score(Range is from 0 to +1):***

**This is the score that determines if a given text is objective or subjective. It is calculated by using the formula:**

**Subjectivity Score = (Positive Score + Negative Score)/ ((Total Words after cleaning) + 0.000001)**
"""

def subjectivity_score(positive_score_lst_articlewise, negative_score_lst_articlewise, articles_wo_stopwords_lst):
    subjectivity_score_articlewise = []
    total_words_count_articlewise = []
    for articles in articles_wo_stopwords_lst:
        words_count = 0
        for words in articles:
            words_count += 1
        total_words_count_articlewise.append(words_count)
    for score in range(len(positive_score_lst_articlewise)):
        #print(score)
        subjectivity_score = (positive_score_lst_articlewise[score] + negative_score_lst_articlewise[score]) / ((total_words_count_articlewise[score]) + 0.000001)
        #print(subjectivity_score)
        subjectivity_score_articlewise.append(subjectivity_score)
    return subjectivity_score_articlewise

subjectivity_score_articlewise = subjectivity_score(positive_score_lst_articlewise, negative_score_lst_articlewise, articles_wo_stopwords_lst)

#print(subjectivity_score_articlewise)

"""# ***Analysis of Readability:***

**Analysis of Readability is calculated using the Gunning Fox index formula described below.**

# ***Average Sentence Length:***

**Average Sentence Length = the number of words / the number of sentences**
"""

def average_sentence_length(article_lst):
    average_sentence_length_articlewise = []
    for article in article_lst:
        number_of_words = len(word_tokenize(article))
        number_of_sentences = len(sent_tokenize(article))
        average_sentence_length = number_of_words / number_of_sentences
        average_sentence_length_articlewise.append(average_sentence_length)
    return average_sentence_length_articlewise

average_sentence_length_articlewise = average_sentence_length(article_lst)

#print(average_sentence_length_articlewise)

"""# ***Percentage of Complex words:***

**Percentage of Complex words = the number of complex words / the number of words**
"""

def percentage_of_complex_words(article_lst):
    vowels = 'aeiou'
    percentage_of_complex_words_articlewise = []
    for article in article_lst:
        article_norm_txt = text_norm(article)
        #print(article_norm_txt)
        article_lst_norm = article_norm_txt.split(' ')
        two_syllables_words = []
        syllables_words = []
        number_of_words = len(word_tokenize(article))
        for word in article_lst_norm:
            count = 0
            for index in range(0,len(word)):
                #print(index)
                if word[index] in vowels:
                    #print(word[index])
                    count += 1
            if word.endswith('es') or word.endswith('ed'):
                count -= 1  
            if count > 2:
                two_syllables_words.append(word)
            else:
                syllables_words.append(word)
        number_of_complex_words = len(two_syllables_words)
        percentage_of_complex_words = number_of_complex_words / number_of_words
        percentage_of_complex_words_articlewise.append(percentage_of_complex_words * 100)
    return percentage_of_complex_words_articlewise

percentage_of_complex_words_articlewise = percentage_of_complex_words(article_lst)

#print(percentage_of_complex_words_articlewise)

"""# ***Fog Index:***

**Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)**
"""

def fog_index(average_sentence_length_articlewise, percentage_of_complex_words_articlewise):
    fog_index_article_wise = []
    for score in range(len(average_sentence_length_articlewise)):
        fog_index = 0.4 * (average_sentence_length_articlewise[score] + percentage_of_complex_words_articlewise[score])
        fog_index_article_wise.append(fog_index)
    return fog_index_article_wise

fog_index_article_wise = fog_index(average_sentence_length_articlewise, percentage_of_complex_words_articlewise)

#print(fog_index_article_wise)

"""# ***Average Number of Words Per Sentence:***

**Average Number of Words Per Sentence = 
            the total number of words / the total number of sentences**
"""

def avg_no_of_words(article_lst):
    average_no_of_words_per_sentence_articlewise = []
    for article in article_lst:
        number_of_words = len(word_tokenize(article))
        number_of_sentences = len(sent_tokenize(article))
        average_no_of_words = number_of_words / number_of_sentences
        average_no_of_words_per_sentence_articlewise.append(average_no_of_words)
    return average_no_of_words_per_sentence_articlewise

average_no_of_words_per_sentence_articlewise = avg_no_of_words(article_lst)

#print(average_no_of_words_per_sentence_articlewise)

"""# ***Complex Word Count:***

**Complex words are words in the text that contain more than two syllables.**
"""

def complex_word(article_lst):
    vowels = 'aeiou'
    number_of_complex_words_articlewise = []
    for article in article_lst:
        article_norm_txt = text_norm(article)
        article_lst_norm = article_norm_txt.split(' ')
        two_syllables_words = []
        for word in article_lst_norm:
            count = 0
            for index in range(0,len(word)):
                #print(index)
                if word[index] in vowels:
                    #print(word[index])
                    count += 1
            if word.endswith('es') or word.endswith('ed'):
                count -= 1  
            if count > 2:
                two_syllables_words.append(word)
        number_of_complex_words = len(two_syllables_words)
        number_of_complex_words_articlewise.append(number_of_complex_words)
    return number_of_complex_words_articlewise

number_of_complex_words_articlewise  = complex_word(article_lst)

#print(number_of_complex_words_articlewise)

"""# ***Word Count:***

**We count the total cleaned words present in the text by:**

1. removing the stop words (using stopwords class of nltk package).
2. removing any punctuations like ? ! , . from the word before counting.
"""

def word_count(article_lst):
    word_count_articlewise_lst = []
    for article in article_lst:
        article_norm_txt = text_norm(article)
        article_lst_norm = article_norm_txt.split(' ')  #article text without punctuations
        article_wo_stopwords = [ word for word in article_lst_norm if word not in stopwords.words('english') ]
        word_count_articlewise_lst.append(len(article_wo_stopwords))
    return word_count_articlewise_lst

word_count_articlewise_lst = word_count(article_lst)

#print(word_count_articlewise_lst)

"""# ***Syllable Count Per Word:***

**We count the number of Syllables in each word of the text by counting the vowels present in each word. We also handle some exceptions like words ending with "es","ed" by not counting them as a syllable.**
"""

def syllable_count(article_lst):
    vowels = 'aeiou'
    syllable_count_per_word_lst_articlewise = []
    for article in article_lst:
        article_norm_txt = text_norm(article)
        article_lst_norm = article_norm_txt.split(' ')
        syllables_words = []
        for word in article_lst_norm:
            #print(word)
            count = 0
            for index in range(0,len(word)):
                if word[index] in vowels:
                    #print(word[index])
                    count += 1
            if word.endswith('es') or word.endswith('ed'):
                count -= 1  
        syllable_count_per_word_lst_articlewise.append(count)
    return syllable_count_per_word_lst_articlewise

syllable_count_per_word_lst_articlewise = syllable_count(article_lst)

#print(syllable_count_per_word_lst_articlewise)

"""# ***Personal Pronouns:***

**To calculate Personal Pronouns mentioned in the text, we use regex to find the counts of the words - “I,” “we,” “my,” “ours,” and “us”.
Special care is taken so that the country name US is not included in the list.**
"""

import re

def personal_pronouns(article_lst):
    personal_pronouns_count_articlewise = []
    for article in article_lst:
        pronounRegex = re.compile(r'\b(I|we|my|ours|us|(?-i:us))\b',re.I)
        pronouns = pronounRegex.findall(article)
        count_for_personal_pronouns = len(pronouns)
        personal_pronouns_count_articlewise.append(count_for_personal_pronouns)
    return personal_pronouns_count_articlewise

personal_pronouns_count_articlewise = personal_pronouns(article_lst)

#print(personal_pronouns__count_articlewise)

"""# ***Average Word Length:***

**Average Word Length is calculated by the formula:**     
        **Sum of the total number of characters in each word / Total number of words**
"""

def average_word_length(article_lst):
    average_word_length_articlewise = []
    for article in article_lst:
        article_norm_txt = text_norm(article)
        article_lst_norm = article_norm_txt.split(' ')
        sum_of_total_no_of_characters = 0
        number_of_words = len(word_tokenize(article))
        for words in article_lst_norm:
            for characters in words:
                sum_of_total_no_of_characters += 1
        average_word_length = sum_of_total_no_of_characters / number_of_words 
        average_word_length_articlewise.append(average_word_length)
    return average_word_length_articlewise

average_word_length_articlewise = average_word_length(article_lst)

#print(average_word_length_articlewise)

"""# ***Creating Out-put Data Structure:***"""

def data_frame(data, positive_score_lst_articlewise,\
               negative_score_lst_articlewise, polarity_score_lst_articlewise,\
               subjectivity_score_articlewise, average_sentence_length_articlewise,\
               percentage_of_complex_words_articlewise, fog_index_article_wise,\
               average_no_of_words_per_sentence_articlewise,\
               number_of_complex_words_articlewise, word_count_articlewise_lst,\
               syllable_count_per_word_lst_articlewise,\
               personal_pronouns_count_articlewise, average_word_length_articlewise):
    output_data_frame = {}
    output_data_frame['URL_ID'] = [IDs for IDs in data['URL_ID']]
    output_data_frame['URLs'] = [urls for urls in data['URL']]
    output_data_frame['POSITIVE SCORE'] = [score for score in positive_score_lst_articlewise]
    output_data_frame['NEGATIVE SCORE'] = [score for score in negative_score_lst_articlewise]
    output_data_frame['POLARITY SCORE'] = [score for score in polarity_score_lst_articlewise]
    output_data_frame['SUBJECTIVITY SCORE'] = [score for score in subjectivity_score_articlewise]
    output_data_frame['AVG SENTENCE LENGTH'] = [length for length in average_sentence_length_articlewise]
    output_data_frame['PERCENTAGE OF COMPLEX WORDS'] = [percentage for percentage in percentage_of_complex_words_articlewise]
    output_data_frame['FOG INDEX'] = [index for index in fog_index_article_wise]
    output_data_frame['AVG NUMBER OF WORDS PER SENTENCE'] = [words for words in average_no_of_words_per_sentence_articlewise]
    output_data_frame['COMPLEX WORD COUNT'] = [count for count in number_of_complex_words_articlewise]
    output_data_frame['WORD COUNT'] = [count for count in word_count_articlewise_lst]
    output_data_frame['SYLLABLE PER WORD'] = [count for count in syllable_count_per_word_lst_articlewise]
    output_data_frame['PERSONAL PRONOUNS'] = [count for count in personal_pronouns_count_articlewise]
    output_data_frame['AVG WORD LENGTH'] = [length for length in average_word_length_articlewise]
    return pd.DataFrame(output_data_frame)

output_dataframe = data_frame(data, positive_score_lst_articlewise, negative_score_lst_articlewise, polarity_score_lst_articlewise, subjectivity_score_articlewise, average_sentence_length_articlewise, percentage_of_complex_words_articlewise, fog_index_article_wise, average_no_of_words_per_sentence_articlewise, number_of_complex_words_articlewise, word_count_articlewise_lst, syllable_count_per_word_lst_articlewise, personal_pronouns_count_articlewise, average_word_length_articlewise)

print(output_dataframe)

"""# ***Saving Output Dataframe in CSV format:***"""

output_dataframe.to_csv('Output_Data.csv',index=False)