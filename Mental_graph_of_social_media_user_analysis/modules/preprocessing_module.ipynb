{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import os\n","import string\n","import re\n","import numpy as np \n","\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords, wordnet\n","from nltk.stem import WordNetLemmatizer\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n"],"metadata":{"id":"P9_Y6LDR1x6R"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jL9OqdfISP2l"},"outputs":[],"source":["#dummy function\n","\n","def cal_sum(a,b):\n","    return a+b"]},{"cell_type":"code","source":["def lower_target_variable(x):\n","    if isinstance(x, str):\n","        return x.lower()\n","    else:\n","        return x    \n","\n","def create_unique_username(prefix, x):\n","    x = prefix+ str(x)\n","    return x\n","    \n","def output_cleaned_individual_csv(temp_df_a, file_name):\n","    csv_op_folder_path = '/content/drive/MyDrive/CDAC-SELF/CDAC_PROJECT/Mental_Graph_Of_User_Using_Social_Media_Posts/project_code/files/cleaned/'\n","    \n","    split_fname = os.path.splitext(file_name)\n","    temp_path = split_fname[0]+'_cleaned'+split_fname[1]\n","\n","    op_path = os.path.join(csv_op_folder_path, temp_path)\n","\n","    ######\n","    temp_df_a = drop_duplicates(temp_df_a)\n","    temp_df_a = drop_na_values_category(temp_df_a)\n","    temp_df_a = cleaning(temp_df_a)\n","    ######\n","    temp_df_a.to_csv(op_path, index = False, header=True)\n","\n","\n","    "],"metadata":{"id":"tIsbQh6kFmai"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mqcAdPxtiB2t"},"outputs":[],"source":["def concat_csvs(csv_folder_path):\n","\n","    dataframes = []\n","    for file in os.listdir(csv_folder_path):\n","        if file.endswith('.csv'):\n","            temp_path = os.path.join(csv_folder_path, file)\n","\n","            temp_df = pd.read_csv(temp_path)\n","            \n","            #1. Sanity Check\n","            #providing neutral name to tweet/comment column\n","            if(file.startswith('tweets')):\n","                temp_df.rename(columns={'Tweet':'orig_sentence'}, inplace = True)\n","                # temp_df.rename(columns={'username':'unique_username'}, inplace = True)\n","                temp_df['username'] = temp_df['username'].apply(lambda x: create_unique_username('tweets_',x) )\n","\n","            elif(file.startswith('ytComments')):\n","                temp_df.rename(columns={'textOriginal':'orig_sentence'}, inplace = True)\n","                temp_df.rename(columns={'authorDisplayName':'username'}, inplace = True)\n","                temp_df['username'] = temp_df['username'].apply(lambda x: create_unique_username('ytComments_',x) )\n","\n","            else:\n","                print(\"Unformatted column name found in file:\", file)  \n","\n","            #2. Sanity Check\n","            #checking both target variables is available or not\n","            if 'category' not in temp_df.columns:\n","                print('\"category\" column not found in file:', file)\n","            elif 'sub-category' not in temp_df.columns:\n","                print('\"sub-category\" column not found in file:', file)\n","\n","\n","            temp_df = temp_df[['username', 'orig_sentence', 'category', 'sub-category']]\n","            \n","\n","            #3. Sanity Check\n","            #lowering target variables\n","            temp_df['category'] = temp_df['category'].apply(lambda x: lower_target_variable(x) )\n","            temp_df['sub-category'] = temp_df['sub-category'].apply(lambda x: lower_target_variable(x))\n","\n","            ###\n","            # print(\"Filename\",file,\"Category\", temp_df['category'].unique())\n","            # print(\"Filename\",file,\"Category\", temp_df['sub-category'].unique())\n","\n","            ###\n","            \n","            ###converting neutral to positive-starts\n","            temp_df.loc[temp_df[\"category\"] == \"neutral\", \"category\"] = \"positive\"\n","            ###converting neutral to positive-ends\n","\n","            #output cleaned individual csv\n","            dataframes.append(temp_df)\n","            # output_cleaned_individual_csv(temp_df, file)\n","            \n","\n","    df = pd.concat(dataframes)\n","    return df \n"]},{"cell_type":"code","source":["\n","def drop_duplicates(df):\n","    df.drop_duplicates() #print it\n","    return df\n","\n","def drop_na_values_category(df):\n","    df.dropna(subset=['category'], inplace=True) #print it\n","    return df\n","\n","def drop_na_values_subcategory(subc_df):\n","    print('drop_na_values_subcategory:', subc_df.shape)\n","    subc_df = subc_df.dropna(subset=['sub-category']) #print it\n","    print('drop_na_values_subcategory:', subc_df.shape)\n","\n","\n","    # drop_subcategory.dropna(subset=['sub-category'], inplace=True) #print it\n","\n","    return subc_df\n","\n","\n","\n","\n","def output_merged_cleaned_csv(df):\n","    df.to_csv(r'/content/drive/MyDrive/CDAC-SELF/CDAC_PROJECT/Mental_Graph_Of_User_Using_Social_Media_Posts/project_code/files/cleaned/merged_cleaned.csv', index = False, header=True)\n","    return None"],"metadata":{"id":"CtlLB-AsnByq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Cleaning\n","\n","def remove_newline(text):\n","    nl = re.compile('\\n')\n","    return nl.sub(r' ', text)\n","\n","def remove_hastag(text):\n","    ht = re.compile('#[^\\s]+')\n","    return ht.sub(r'',text)\n","    \n","\n","def remove_at_symbol(text):\n","    at = re.compile('@[^\\s]+')\n","    return at.sub(r'',text)\n","    \n","\n","def remove_URL(text):\n","    url = re.compile(r'https?://\\S+|www\\.\\S+')\n","    return url.sub(r'', text)\n","    \n","\n","def remove_emoji(text):\n","    emoji_pattern = re.compile(\n","        '['\n","        u'\\U0001F600-\\U0001F64F'  # emoticons\n","        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n","        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n","        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n","        u'\\U00002702-\\U000027B0'\n","        u'\\U000024C2-\\U0001F251'\n","        ']+',\n","        flags=re.UNICODE)\n","    return emoji_pattern.sub(r'', text)\n","\n","\n","def remove_html(text):\n","    html = re.compile(r'^[^ ]<.*?>|&([a-z0-9]+|#[0-9]\\\"\\'\\â€œ{1,6}|#x[0-9a-f]{1,6});[^A-Za-z0-9]+')\n","    return re.sub(html, '', text)\n","\n","\n","def remove_punct(text):\n","    table = str.maketrans('', '', string.punctuation)\n","    return text.translate(table)\n","\n","def remove_quotes(text):\n","    quotes = re.compile(r'[^A-Za-z0-9\\s]+')\n","    return re.sub(quotes, '', text)\n","\n"],"metadata":{"id":"5JRxOXXXnBu1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def applying_helper_functions(df):\n","\n","    df['mod_text'] = df['orig_sentence'].apply(lambda x: remove_newline(x))\n","    df['mod_text'] = df['mod_text'].apply(lambda x: remove_hastag(x))\n","    df['mod_text'] = df['mod_text'].apply(lambda x: remove_at_symbol(x))\n","    df['mod_text'] = df['mod_text'].apply(lambda x: remove_URL(x))\n","    df['mod_text'] = df['mod_text'].apply(lambda x: remove_emoji(x))\n","    df['mod_text'] = df['mod_text'].apply(lambda x: remove_html(x))\n","    df['mod_text'] = df['mod_text'].apply(lambda x: remove_punct(x))\n","    df['mod_text'] = df['mod_text'].apply(lambda x: remove_quotes(x))\n","\n","    return df\n"],"metadata":{"id":"UdRnQrxjnBp5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_wordnet_pos(tag):\n","    if tag.startswith('J'):\n","        return wordnet.ADJ\n","    elif tag.startswith('V'):\n","        return wordnet.VERB\n","    elif tag.startswith('N'):\n","        return wordnet.NOUN\n","    elif tag.startswith('R'):\n","        return wordnet.ADV\n","    else:\n","        return wordnet.NOUN"],"metadata":{"id":"miQdpUxxnBnB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def stopwords_setup(stopwords):\n","\n","    stop_words = set(stopwords.words(\"english\"))\n","    stop_words.remove('not')\n","    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n","    stop_words = stop_words.union(more_stopwords)\n","\n","    return stop_words"],"metadata":{"id":"oC80er1e7DGn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def select_columns(df, prediction=True):\n","    if prediction:\n","        df = df[['username','orig_sentence',\"cleaned_sentence\"]]\n","    else:    \n","        df = df[['username','orig_sentence',\"cleaned_sentence\", \"category\", \"sub-category\"]]\n","    return df"],"metadata":{"id":"0hm2t1tGwk_p"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oGBoelpHocgg"},"outputs":[],"source":["# Tokenizing the tweet base texts.\n","\n","def cleaning(df, prediction=False):\n","\n","    if prediction:\n","        df['category'] = None\n","        df['sub-category'] = None\n","        #for cleaning part prediction -starts\n","        if 'Tweet' in df.columns:\n","            df.rename(columns={'Tweet':'orig_sentence'}, inplace = True)\n","            print('column renamed: Tweet orig_sentence')\n","\n","        if 'textOriginal' in df.columns:\n","            df.rename(columns={'textOriginal':'orig_sentence'}, inplace = True)\n","            print('columns renamed textOriginal orig_sentence')\n","        #for cleaning part prediction -ends\n","\n","    df = applying_helper_functions(df)\n","\n","    stop_words = stopwords_setup(stopwords)\n","    df['tokenized'] = df['mod_text'].apply(word_tokenize)\n","    df['lower'] = df['tokenized'].apply(lambda x: [word.lower() for word in x])\n","    df['stopwords_removed'] = df['lower'].apply(lambda x: [word for word in x if word not in stop_words])\n","    df['pos_tags'] = df['stopwords_removed'].apply(nltk.tag.pos_tag)\n","    df['wordnet_pos'] = df['pos_tags'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\n","\n","    wnl = WordNetLemmatizer()\n","\n","    df['lemmatized'] = df['wordnet_pos'].apply(lambda x: [wnl.lemmatize(word, tag) for word, tag in x])\n","    df['lemmatized'] = df['lemmatized'].apply(lambda x: [word for word in x if word not in stop_words])\n","    df['cleaned_sentence'] = [' '.join(map(str, l)) for l in df['lemmatized']]\n","\n","    \n","    df = select_columns(df ,prediction=False)\n","    # print(df.shape)\n","    df['cleaned_sentence'].replace('', np.nan, inplace=True)\n","    df.dropna(subset=['cleaned_sentence'], inplace=True)\n","    # print(df.shape)\n","\n","    return df\n","# df.head()"]},{"cell_type":"code","source":["def splitting_train_test(df,TRAIN_SIZE):\n","    df_train, df_test = train_test_split(df, test_size=1-TRAIN_SIZE, random_state=42)\n","    print(\"TRAIN size:\", len(df_train))\n","    print(\"TEST size:\", len(df_test))\n","\n","    return df_train, df_test"],"metadata":{"id":"JxALSeT9shFo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def labelEncoding_target_variables(df_train, df_test):\n","    encoder = LabelEncoder()\n","    encoder.fit(df_train.category.tolist())\n","\n","    y_train = encoder.transform(df_train.category.tolist())\n","    y_test = encoder.transform(df_test.category.tolist())\n","\n","    y_train = y_train.reshape(-1,1)\n","    y_test = y_test.reshape(-1,1)\n","\n","    print(\"y_train\",y_train.shape)\n","    print(\"y_test\",y_test.shape)\n","\n","    return encoder, y_train, y_test"],"metadata":{"id":"VQeANRG4shB4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tokenizing(df_train):\n","    tokenizer = Tokenizer()\n","    tokenizer.fit_on_texts(df_train.cleaned_sentence)\n","\n","    vocab_size = len(tokenizer.word_index) + 1\n","    # print(\"Total words\", vocab_size)\n","\n","    return tokenizer\n","\n","def converting_txt2seq(tokenizer, df_train, df_test, SEQUENCE_LENGTH):\n","    x_train = pad_sequences(tokenizer.texts_to_sequences(df_train.cleaned_sentence), maxlen=SEQUENCE_LENGTH)\n","    x_test = pad_sequences(tokenizer.texts_to_sequences(df_test.cleaned_sentence), maxlen=SEQUENCE_LENGTH)\n","\n","    return x_train, x_test"],"metadata":{"id":"bucV_d-Zsg-L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"26EklCsB-F-o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Y8yn1D5F-F7g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5QMHq2C_sg7s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["###### WORD2VEC "],"metadata":{"id":"ZyBlAn215XGY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Parameters for WORD2VEC\n","W2V_SIZE = 50\n","W2V_WINDOW = 7\n","W2V_EPOCH = 50\n","W2V_MIN_COUNT = 3"],"metadata":{"id":"EsdJK2LYhnFf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from gensim.models import Word2Vec\n","\n","def create_Word2Vec_embedding(df_train):\n","    \n","    documents = [text.split() for text in df_train.cleaned_sentence] \n","    w2v_model = Word2Vec(size=W2V_SIZE, window=W2V_WINDOW, min_count=W2V_MIN_COUNT, workers=8)\n","    w2v_model.build_vocab(documents)\n","    w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)\n","\n","    w2v_weights = w2v_model.wv.vectors\n","    w2v_vocab_size, w2v_embedding_size = w2v_weights.shape\n","\n","    return w2v_model, w2v_weights, w2v_vocab_size, w2v_embedding_size\n"],"metadata":{"id":"4_gAJWo45XLL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_embeddingMatrix_from_Word2Vec(w2v_model, vocab_size, W2V_SIZE, tokenizer):\n","\n","    embedding_matrix = np.zeros((vocab_size, W2V_SIZE))\n","    for word, i in tokenizer.word_index.items():\n","        if word in w2v_model.wv:\n","            embedding_matrix[i] = w2v_model.wv[word]\n","        \n","    return embedding_matrix"],"metadata":{"id":"3NNbPhrNRuYQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"j059oqKESV40"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"iongCOa1SVyD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Wqr4RtQsSVpR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VmxprperSVkQ"},"execution_count":null,"outputs":[]}]}