# -*- coding: utf-8 -*-
"""main_bert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NAfs7kp2BZorPo_9OtPSm0PS9dKjqOqp
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

def plot_model_performance_graph(hist_df):
    
    fig, axes = plt.subplots(1,2 , figsize = (15,6))

    # properties  matplotlib.patch.Patch 
    props = dict(boxstyle='round', facecolor='aqua', alpha=0.4)

    ax = axes[0]

    hist_df.plot(y = ['loss','val_loss'], ax = ax, colormap="cividis")

    lossmin = hist_df['loss'].min()

    testmin = hist_df['val_loss'].min()

    # little beautification
    txtstr = "Min Loss: \n Training : {:7.4f}\n Testing   : {:7.4f}".format(lossmin,
                                                                            testmin) #text to plot

    # place a text box in upper left in axes coords

    ax.text(0.4, 0.95, txtstr, transform=ax.transAxes, fontsize=14,
            verticalalignment='top', bbox=props)

    ax.set_xlabel("Epochs")

    ax.set_ylabel("Loss")

    ax.grid();

    ax = axes[1]

    hist_df.plot( y = ['sparse_categorical_accuracy', 'val_sparse_categorical_accuracy'], ax = ax, colormap="cividis")
    acc = hist_df['sparse_categorical_accuracy'].values
    # little beautification
    accmax = hist_df['sparse_categorical_accuracy'].max()
    testmax = hist_df['val_sparse_categorical_accuracy'].max()
    txtstr = "Max Accuracy: \n Training : {:7.4f}\n Testing   : {:7.4f}".format(accmax,
                                                                                testmax) #text to plot

    # place a text box in upper left in axes coords

    ax.text(0.4, 0.5, txtstr, transform=ax.transAxes, fontsize=14,
            verticalalignment='top', bbox=props)

    ax.set_xlabel("Epochs")
    ax.set_ylabel("Accuracy")
    ax.grid();
    plt.tight_layout()

!pip install -q -U "tensorflow-text==2.8.*"

from google.colab import drive
drive.mount('/content/drive')

pip install -q tf-models-official==2.7.0

pip install nlpaug

import functools
import os
import shutil
import tensorflow_text as text
import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text
from official.nlp import optimization  # to create AdamW optimizer
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

"""# Importing the dataset"""

AUTOTUNE = tf.data.AUTOTUNE
seed = 42
train_df = pd.read_csv('/content/merged_cleaned.csv')

print(train_df.head())
train_df.columns

X_train = train_df.loc[train_df['category'] == 'negative']

def changen(x):
    if x == 'critisism':
        return 'criticism'
    elif x == 'anger':
        return 'hate'
    elif x == 'sarcasm' or x == 'sadness':
        return 'criticism'
    elif x == 'offencive':
        return 'hate'
    elif x == 'anti-national':
        return 'hate'
    else:
        return x

X_train['sub-category'] = X_train['sub-category'].apply(changen)

X_train['sub-category'].value_counts()

X_train.to_csv('Tweets.csv',index=None)



X_train

X_aug = X_train[X_train['sub-category'] == 'hate']

X_aug.reset_index(drop=True,inplace =True)

import nlpaug.augmenter.word as naw
aug = naw.SynonymAug(aug_src='wordnet',aug_max=6)
lst = []
for i in X_aug.cleaned_sentence:
  lst.append(aug.augment(i, n=6))

lst2 = []
for i in lst:
  for j in i:
    lst2.append(j)



aug_df = pd.DataFrame({"cleaned_sentence" : lst2 , "category" : ['negative' for i in range(len(lst2))] ,"sub-category" : ['hate' for i in range(len(lst2))]} )

aug_df



"""# Main Code stars here

"""





X_train = pd.read_csv('./Tweets.csv')

X_train.head()

X_train.dropna(axis = 0, how ='any',inplace=True)

X = pd.concat([X_train['cleaned_sentence'] , aug_df['cleaned_sentence']])

y = pd.concat([X_train['sub-category'], aug_df['sub-category']])

y.value_counts()



lb = LabelEncoder()
y_l = lb.fit_transform(y)





lb.classes_



tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/2'
tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'



from sklearn.model_selection import train_test_split

X_t ,X_ts , y_t,y_ts = train_test_split(X,y_l,shuffle=True,test_size=0.25,random_state=2022,stratify=y)

X_t.shape ,X_ts.shape , y_t.shape,y_ts.shape

X_t = tf.convert_to_tensor(X_t)
X_ts = tf.convert_to_tensor(X_ts)
y_t = tf.convert_to_tensor(y_t)
y_ts = tf.convert_to_tensor(y_ts)

y_t

BATCH_SIZE = 16
SHUFFLE_BUFFER_SIZE = 100

def build_classifier_model():
  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')
  encoder_inputs = preprocessing_layer(text_input)
  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')
  outputs = encoder(encoder_inputs)
  net = outputs['pooled_output']
  net = tf.keras.layers.Dropout(0.1)(net)
  net =  tf.keras.layers.Dense(64, activation='relu')(net)
  net = tf.keras.layers.Dense(2, activation='softmax', name='classifier')(net)
  return tf.keras.Model(text_input, net)

classifier_model = build_classifier_model()

loss = tf.keras.losses.SparseCategoricalCrossentropy()
metrics = tf.metrics.SparseCategoricalAccuracy()

X.shape[0]

epochs = 10
steps_per_epoch = X.shape[0]//16
num_train_steps = steps_per_epoch * epochs
num_warmup_steps = int(0.1*num_train_steps)

init_lr = 3e-5
optimizer = optimization.create_optimizer(init_lr=init_lr,
                                          num_train_steps=num_train_steps,
                                          num_warmup_steps=num_warmup_steps,
                                          optimizer_type='adamw')

classifier_model.compile(optimizer=optimizer,
                         loss=loss,
                         metrics=metrics)

print(f'Training model with {tfhub_handle_encoder}')
history = classifier_model.fit(x=X_t,
                               y = y_t,
                               shuffle=True,
                               validation_data=(X_ts,y_ts),
                               batch_size=BATCH_SIZE,
                               epochs=epochs)

classifier_model.save("/content/smallBert" ,include_optimizer=False)

classifier_model.summary()



loss_df = pd.DataFrame(history.history)

tf.keras.utils.plot_model(classifier_model)

plot_model_performance_graph(loss_df)

y_out = tf.nn.softmax(classifier_model.predict(['it should not be like that its wrong ']))

y_out

y_lab = np.argmax(y_out.numpy(),axis = 1)

y_clsses = lb.inverse_transform(y_lab)

y_clsses

submission = pd.DataFrame({
    'id': test_df.iloc[:,0],
    'labels': y_clsses
})

submission.to_csv('submission.csv',index=False)

# dataset_name = 'tweets'
# saved_model_path = './{}_bert.h5'.format(dataset_name.replace('/', '_'))

# classifier_model.save(saved_model_path, include_optimizer=False)

from visualization_module import  *

y_test = tf.nn.softmax(classifier_model.predict(X_ts))
y_lab = np.argmax(y_test.numpy(),axis = 1)

from sklearn.metrics import confusion_matrix,classification_report

matrix = confusion_matrix(y_ts,y_lab)

plot_confusion_matrix(matrix,['Criticism' , 'Hate'])

cr = classification_report(y_ts,y_lab)

print(cr)

