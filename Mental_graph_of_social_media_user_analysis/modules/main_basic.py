# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hSNgFu5tdQuwNWj0MOQGfMP05D1BZNPT

## **Contents of the Workbook :**
"""

from google.colab import drive
drive.mount('/content/drive')

"""## **Step - 1 :** *Importing Necesaary Libraries and Datasets*

### **Step - 1.1 :** *Importing Necesaary Packages & Libraries*
"""

!pip install import_ipynb
!pip install -q -U "tensorflow-text==2.8.*"

# Commented out IPython magic to ensure Python compatibility.
# DataFrame
import pandas as pd

# Matplot
import matplotlib.pyplot as plt
# %matplotlib inline

from matplotlib.ticker import MaxNLocator
import matplotlib.gridspec as gridspec
import matplotlib.patches as mpatches

# Scikit-learn

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.manifold import TSNE
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation, NMF
from sklearn.metrics import f1_score, accuracy_score


# Keras
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM, Bidirectional
from tensorflow.keras import utils
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping

# nltk
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
from nltk.corpus import stopwords, wordnet
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
import nltk
nltk.download('omw-1.4')

# Word2vec
import gensim
from gensim.test.utils import common_texts
from gensim.models import Word2Vec


# Utility
import string
import re
import numpy as np
import os
import logging
import time
import pickle
import itertools
import random
import datetime

# WordCloud
from PIL import Image
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
from collections import Counter, defaultdict

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Warnings
import warnings 
warnings.filterwarnings('ignore')

# Set log
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)



# %cd '/content/drive/MyDrive/CDAC-SELF/CDAC_PROJECT/Mental_Graph_Of_User_Using_Social_Media_Posts/project_code/src/'

import import_ipynb
import preprocessing_module as preprocessing
import visualization_module as visualization

"""### **Step - 1.2 :** *Declaring Variables*"""

# Varaibles related to dataset

TRAIN_SIZE = 0.8

# Parameters related to KERAS
SEQUENCE_LENGTH = 150
EPOCHS = 20
BATCH_SIZE = 64

# Variables for Exporting purpose
KERAS_MODEL = "model.h5"
WORD2VEC_MODEL = "model.w2v"
TOKENIZER_MODEL = "/content/tokenizer.pkl"
ENCODER_MODEL = "encoder.pkl"

# MISC
plt.style.use('fivethirtyeight')
pd.options.display.max_columns = 250
pd.options.display.max_rows = 250

csv_folder_path = '/content/drive/MyDrive/CDAC-SELF/CDAC_PROJECT/Mental_Graph_Of_User_Using_Social_Media_Posts/project_code/files/labelled'
model_path = '/content/drive/MyDrive/CDAC-SELF/CDAC_PROJECT/Mental_Graph_Of_User_Using_Social_Media_Posts/project_code/src/model/final/model.h5'
TOKENIZER_MODEL = "/content/drive/MyDrive/CDAC-SELF/CDAC_PROJECT/Mental_Graph_Of_User_Using_Social_Media_Posts/project_code/src/model/tokenizer.pkl"

"""## **Step - 2 :** *Data Pre-processing*

### **Step - 2.1 :** Import Cleaning functions from Preprocessing module file
"""

##preprocessing-starts

df = preprocessing.concat_csvs(csv_folder_path)
df = preprocessing.drop_duplicates(df)
df = preprocessing.drop_na_values_category(df)
df = preprocessing.cleaning(df)

df.head()

"""### **Step - 2.2 :** Outputting Cleaned CSV File"""

#outputting merged cleaned csv
preprocessing.output_merged_cleaned_csv(df)

print("DataFrame Shape:",df.shape)
df.describe()

#preprocessing-ends

"""### **Step - 2.3 :** Plotting Class Distribution"""

#Plotting Class Distribution-part1

#Calling Visualization function from Visualization module File

visualization.plot_category_counts(df['category'], title = "Category Distribution")

#Plotting Class Distribution-part2

import copy
subc_df = copy.deepcopy(df)
subc_df = preprocessing.drop_na_values_subcategory(subc_df)

visualization.plot_category_counts(subc_df['sub-category'],  title = "Sub-Category Distribution")

"""### **Step - 2.3 :** Plotting Word Cloud"""

def plot_wordcloud(df):

    # text = open('/content/drive/MyDrive/CDAC-SELF/CDAC_PROJECT/Mental_Graph_Of_User_Using_Social_Media_Posts/project_code/files/cleaned/merged_cleaned.csv').read()
    
    # text = df['cleaned_sentence'].astype("string")
    # text = str(df.cleaned_sentence)
    text = str(df['cleaned_sentence'].values)


    wordcloud = WordCloud().generate(text)


    # Generate plot
    plt.figure(figsize=(12,10))
    plt.imshow(wordcloud)
    plt.axis("off")
    plt.show()

plot_wordcloud(df)

"""### **Step - 2.4 :** Train-Test Split"""

df_train, df_test = preprocessing.splitting_train_test(df, TRAIN_SIZE)

encoder, y_train, y_test = preprocessing.labelEncoding_target_variables(df_train, df_test)

"""## **Step - 3 :** *Applying Word2Vec Embedding*"""

#word2vec
W2V_SIZE=200 #Embedding size
W2V_EPOCH=60 

w2v_model, w2v_weights, w2v_vocab_size, w2v_embedding_size = preprocessing.create_Word2Vec_embedding(df_train, W2V_EPOCH, W2V_SIZE)

print("Vocabulary Size: {} - Embedding Dim: {}".format(w2v_vocab_size, w2v_embedding_size))
print('Word2Vec_weights Shape:',w2v_weights.shape)

#testing similar words from word2vec embeddings
w2v_model.wv.most_similar("minister")

w2v_model.wv.most_similar("narendra")

"""### **Step - 5.1 :** Token and Vocab Creation"""

#Creating Token from words
vocab_size, tokenizer = preprocessing.tokenizing(df_train)

#Converting Text to sequence and added padding
x_train, x_test = preprocessing.converting_txt2seq(tokenizer, df_train, df_test, SEQUENCE_LENGTH)

print("x_train", x_train.shape)
print("y_train", y_train.shape)
print()
print("x_test", x_test.shape)
print("y_test", y_test.shape)

"""## STEP 5.1.2 Visualizing Word2Vec Embeddings with t-SNE"""

plot_n_words = 150
visualization.plot_word2vec_embedding_by_tsne(w2v_model, w2v_vocab_size, plot_n_words)

"""### **Step - 5.3 :** Embedding Layer Creation"""

embedding_matrix = preprocessing.create_embeddingMatrix_from_Word2Vec(w2v_model, vocab_size, W2V_SIZE, tokenizer)





"""### **Step - 5.4 :** Model Creation - LSTM"""

model = Sequential()
model.add(Embedding(input_dim=vocab_size,
                    output_dim=w2v_embedding_size,
                    weights=[embedding_matrix],
                    input_length=SEQUENCE_LENGTH,
                    mask_zero=True,
                    trainable=False))
model.add(Dropout(0.2))

model.add(Bidirectional(LSTM(8, dropout=0.2)))

model.add(Dense(1, activation='sigmoid'))

model.summary()

tf.keras.utils.plot_model(model)

"""### **Step - 5.5 :** Compiling Model"""

model.compile(loss='binary_crossentropy', optimizer="adam", metrics=['accuracy'])

callbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=2, cooldown=0),
              EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=5)]

"""### **Step - 5.7 :** Model Training """

# Commented out IPython magic to ensure Python compatibility.
# %%time
# history = model.fit(x_train, y_train,
#                     batch_size=64,
#                     epochs=20,
#                     validation_split=0.1,
#                     verbose=1
#                     )

"""### **Step - 5.8 :** Model Evaluation """

# Commented out IPython magic to ensure Python compatibility.
# %%time
# score = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)
# print()
# print("ACCURACY:",score[1])
# print("LOSS:",score[0])

"""Plotting Model Performance"""

hist_df = pd.DataFrame(history.history)
visualization.plot_model_performance_graph(hist_df)

#Saving Model

model.save(model_path)
# w2v_model.save(WORD2VEC_MODEL)
pickle.dump(tokenizer, open(TOKENIZER_MODEL, "wb"), protocol=0)
# pickle.dump(encoder, open(ENCODER_MODEL, "wb"), protocol=0)

model_loaded = tf.keras.models.load_model(model_path)

model_loaded.summary()

def pred_score2label(y_pred_score):
    y_pred_conf_score,y_pred_num , y_pred_label = [], [], []
    for i in y_pred_score:
        if i<=0.5:
            pred_label = 0
            pred_score = i + 0.5
        else:
            pred_label = 1
            pred_score = i

        y_pred_num.append(pred_label)
        y_pred_conf_score.append(pred_score)

        pred_li_temp = [pred_label]
        y_pred_temp =  list(encoder.inverse_transform(pred_li_temp) )

        y_pred_label.append(y_pred_temp)
    return y_pred_conf_score, y_pred_num, y_pred_label

y_pred_score = model_loaded.predict(x_test)
y_pred_conf_score,y_pred_num, y_pred_label = pred_score2label(y_pred_score)


    # print(type(i))

# (y_pred_num[:4], y_pred_label[:4])
y_test = [i[0] for i in y_test]
y_test[:4], y_pred_conf_score[:4]

# x_test = [i[0] for i in x_test]

(x_test)

def output_pred_csv(df_test, y_pred_conf_score, y_pred_label):

    sent = df_test.cleaned_sentence.tolist()
    y_pred_conf_score = [i[0] for i in y_pred_conf_score]
    y_pred_label = [i[0] for i in y_pred_label]

    pred_df = pd.DataFrame(list(zip(sent, y_pred_conf_score, y_pred_label)), columns =['Sentence', 'confidence_score', 'y_pred_label'])
    pred_df.to_csv(r'/content/pred_df.csv', index=False)

    return pred_df

output_pred_csv(df_test, y_pred_conf_score, y_pred_label)



classification_report = classification_report(y_test, y_pred_num) 
print(classification_report)

confusion_mtx  = confusion_matrix(y_test, y_pred_num)

visualization.plot_confusion_matrix(confusion_mtx, classes =['negative', 'positive'])

